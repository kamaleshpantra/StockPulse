{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNP8VWlh+mFkqHNNnodgeOX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamaleshpantra/StockPulse/blob/main/Stockpulse_trend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_packages():\n",
        "    packages = [\n",
        "        \"pyflink\",\n",
        "        \"praw==7.8.1\",\n",
        "        \"yfinance==0.2.54\",\n",
        "        \"streamlit==1.43.2\",\n",
        "        \"pyngrok==7.2.3\",\n",
        "        \"nltk==3.9.1\",\n",
        "        \"pandas==2.2.2\",\n",
        "        \"matplotlib==3.10.0\",\n",
        "        \"plotly==5.24.1\",\n",
        "        \"scikit-learn==1.6.1\",\n",
        "        \"tensorflow==2.18.0\"\n",
        "    ]\n",
        "    for pkg in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "            print(f\"Installed {pkg}\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Failed to install {pkg}: {e}\")\n",
        "            raise\n",
        "\n",
        "install_packages()\n",
        "print(\"All dependencies installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdzaXAaF-xPt",
        "outputId": "09ed2239-4b5c-4ed9-da2d-ed0c92706f54"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed pyflink\n",
            "Installed praw==7.8.1\n",
            "Installed yfinance==0.2.54\n",
            "Installed streamlit==1.43.2\n",
            "Installed pyngrok==7.2.3\n",
            "Installed nltk==3.9.1\n",
            "Installed pandas==2.2.2\n",
            "Installed matplotlib==3.10.0\n",
            "Installed plotly==5.24.1\n",
            "Installed scikit-learn==1.6.1\n",
            "Installed tensorflow==2.18.0\n",
            "All dependencies installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import nltk\n",
        "import praw\n",
        "import yfinance as yf\n",
        "from pyflink.table import EnvironmentSettings, TableEnvironment, StreamTableEnvironment\n",
        "from pyflink.table.udf import udf\n",
        "from pyflink.datastream import StreamExecutionEnvironment\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import plotly.graph_objects as go\n",
        "import streamlit as st\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    filename=\"/content/stock_predict.log\",\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "\n",
        "# Define sector\n",
        "sectors = {\"Technology\": [\"AAPL\", \"MSFT\", \"GOOGL\", \"TSLA\"]}\n",
        "\n",
        "# Initialize Reddit API with Colab Secrets\n",
        "try:\n",
        "    client_id = userdata.get(\"REDDIT_CLIENT_ID\")\n",
        "    client_secret = userdata.get(\"REDDIT_CLIENT_SECRET\")\n",
        "    user_agent = userdata.get(\"REDDIT_USER_AGENT\") or \"StockSentiment/1.0\"\n",
        "    if not all([client_id, client_secret, user_agent]):\n",
        "        raise ValueError(\"One or more Reddit credentials are missing\")\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=client_id,\n",
        "        client_secret=client_secret,\n",
        "        user_agent=user_agent,\n",
        "        read_only=True,\n",
        "        requestor_kwargs={\"timeout\": 10}\n",
        "    )\n",
        "    limits = reddit.auth.limits\n",
        "    logger.info(\"Reddit API initialized. Rate limit remaining: %s\", limits.get(\"remaining\", \"unknown\"))\n",
        "    print(\"Reddit API initialized\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to initialize Reddit API: {e}\")\n",
        "    raise ValueError(\n",
        "        \"Reddit API setup failed. Ensure REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, \"\n",
        "        \"and REDDIT_USER_AGENT are set in Colab Secrets.\"\n",
        "    )\n",
        "\n",
        "# Initialize VADER\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Define date range\n",
        "end_date = datetime.now().date()\n",
        "start_date = end_date - timedelta(days=365)\n",
        "\n",
        "# Helper for trading days\n",
        "def next_trading_day(date):\n",
        "    \"\"\"Map to next trading day (skip weekends).\"\"\"\n",
        "    date = pd.to_datetime(date)\n",
        "    while date.weekday() >= 5:  # Saturday or Sunday\n",
        "        date += timedelta(days=1)\n",
        "    return date.date()\n",
        "\n",
        "logger.info(\"Setup complete\")\n",
        "print(\"Setup complete!\")\n",
        "print(start_date,end_date)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yvwX_ICi_ED",
        "outputId": "85c86db1-1665-47ee-8228-52367543b62d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reddit API initialized\n",
            "Setup complete!\n",
            "2024-04-16 2025-04-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def fetch_large_reddit(sector, subreddits=[\"wallstreetbets\", \"stocks\", \"investing\"], limit_per_query=500):\n",
        "    \"\"\"\n",
        "    Fetch Reddit posts for sector companies within date range, ensuring relevance.\n",
        "\n",
        "    Args:\n",
        "        sector (str): Sector name (e.g., 'Technology').\n",
        "        subreddits (list): Subreddits to search.\n",
        "        limit_per_query (int): Max posts per subreddit.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to JSONL file.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Fetching Reddit data for {sector}\")\n",
        "    filename = f\"/content/reddit_{sector}_large.jsonl\"\n",
        "\n",
        "    if os.path.exists(filename):\n",
        "        logger.info(f\"Using cached Reddit data: {filename}\")\n",
        "        print(f\"Using cached Reddit data: {filename}\")\n",
        "        return filename\n",
        "\n",
        "    companies = sectors[sector]\n",
        "    all_posts = []\n",
        "    start_timestamp = datetime.combine(start_date, datetime.min.time()).timestamp()\n",
        "    end_timestamp = datetime.combine(end_date + timedelta(days=1), datetime.min.time()).timestamp() - 1\n",
        "\n",
        "    for company in companies:\n",
        "        for subreddit in subreddits:\n",
        "            for attempt in range(3):  # Retry up to 3 times\n",
        "                try:\n",
        "                    logger.info(f\"Attempt {attempt+1}: Fetching {company} from r/{subreddit}\")\n",
        "                    print(f\"Fetching {company} from r/{subreddit} (Attempt {attempt+1})...\")\n",
        "                    submissions = reddit.subreddit(subreddit).search(\n",
        "                        query=company, sort=\"new\", limit=limit_per_query, time_filter=\"year\"\n",
        "                    )\n",
        "                    post_count = 0\n",
        "                    for submission in submissions:\n",
        "                        if start_timestamp <= submission.created_utc <= end_timestamp:\n",
        "                            text = submission.title + \" \" + (submission.selftext or \"\")\n",
        "                            if company.lower() in text.lower():\n",
        "                                post = {\n",
        "                                    \"company\": company,\n",
        "                                    \"text\": text,\n",
        "                                    \"timestamp\": submission.created_utc,\n",
        "                                    \"subreddit\": subreddit\n",
        "                                }\n",
        "                                all_posts.append(post)\n",
        "                                post_count += 1\n",
        "                    logger.info(f\"Fetched {post_count} posts for {company} in r/{subreddit}\")\n",
        "                    print(f\"Fetched {post_count} posts for {company} in r/{subreddit}\")\n",
        "                    break  # Success, move to next subreddit\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Attempt {attempt+1} failed for {company} in {subreddit}: {e}\")\n",
        "                    print(f\"Error for {company} in {subreddit}: {e}\")\n",
        "                    if attempt < 2:\n",
        "                        time.sleep(5)  # Wait before retry\n",
        "                    continue\n",
        "                finally:\n",
        "                    time.sleep(2)  # Rate limit delay\n",
        "\n",
        "    if not all_posts:\n",
        "        logger.warning(f\"No Reddit posts fetched for {sector}\")\n",
        "        print(f\"No Reddit posts fetched for {sector}\")\n",
        "        return filename\n",
        "\n",
        "    with open(filename, \"w\") as f:\n",
        "        for post in all_posts:\n",
        "            f.write(json.dumps(post) + \"\\n\")\n",
        "\n",
        "    logger.info(f\"Saved {len(all_posts)} posts to {filename}\")\n",
        "    print(f\"Saved {len(all_posts)} posts to {filename}\")\n",
        "    return filename\n",
        "\n",
        "sector = \"Technology\"\n",
        "reddit_file = fetch_large_reddit(sector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxWSbZe3NOtj",
        "outputId": "91f7731f-6db6-44df-f003-9157489cc0ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching AAPL from r/wallstreetbets (Attempt 1)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 112 posts for AAPL in r/wallstreetbets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching AAPL from r/stocks (Attempt 1)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 168 posts for AAPL in r/stocks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching AAPL from r/investing (Attempt 1)...\n",
            "Fetched 80 posts for AAPL in r/investing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching MSFT from r/wallstreetbets (Attempt 1)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 130 posts for MSFT in r/wallstreetbets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching MSFT from r/stocks (Attempt 1)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 120 posts for MSFT in r/stocks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching MSFT from r/investing (Attempt 1)...\n",
            "Fetched 99 posts for MSFT in r/investing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching GOOGL from r/wallstreetbets (Attempt 1)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 202 posts for GOOGL in r/wallstreetbets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching GOOGL from r/stocks (Attempt 1)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 247 posts for GOOGL in r/stocks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching GOOGL from r/investing (Attempt 1)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 241 posts for GOOGL in r/investing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching TSLA from r/wallstreetbets (Attempt 1)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 146 posts for TSLA in r/wallstreetbets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching TSLA from r/stocks (Attempt 1)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 205 posts for TSLA in r/stocks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching TSLA from r/investing (Attempt 1)...\n",
            "Fetched 74 posts for TSLA in r/investing\n",
            "Saved 1824 posts to /content/reddit_Technology_large.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import yfinance as yf\n",
        "\n",
        "def fetch_stock_data(sector, cache_dir=\"/content/data\"):\n",
        "    \"\"\"Fetch stock prices for sector companies within date range.\"\"\"\n",
        "    logger.info(f\"Fetching stock data for {sector}\")\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    companies = sectors[sector]\n",
        "\n",
        "    for company in companies:\n",
        "        cache_file = f\"{cache_dir}/stock_{company}.csv\"\n",
        "        if os.path.exists(cache_file):\n",
        "            logger.info(f\"Using cached stock data for {company}\")\n",
        "            print(f\"Using cached stock data for {company}\")\n",
        "            continue\n",
        "        try:\n",
        "            print(f\"Fetching stock data for {company}...\")\n",
        "            stock = yf.Ticker(company)\n",
        "            hist = stock.history(start=start_date, end=end_date + timedelta(days=1), interval=\"1d\")\n",
        "            if hist.empty:\n",
        "                logger.warning(f\"No stock data for {company}\")\n",
        "                print(f\"No stock data for {company}\")\n",
        "                continue\n",
        "            hist = hist.reset_index()[[\"Date\", \"Open\", \"Close\", \"High\", \"Low\", \"Volume\"]]\n",
        "            hist[\"Date\"] = pd.to_datetime(hist[\"Date\"]).dt.date\n",
        "            hist.to_csv(cache_file, index=False)\n",
        "            logger.info(f\"Saved {len(hist)} days for {company} to {cache_file}\")\n",
        "            print(f\"Saved {len(hist)} days for {company}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching stock data for {company}: {e}\")\n",
        "            print(f\"Error for {company}: {e}\")\n",
        "            continue\n",
        "\n",
        "fetch_stock_data(sector=\"Technology\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1JB7sRf6PjA",
        "outputId": "9505d21c-96ec-4b8f-8391-821e7fe1671a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching stock data for AAPL...\n",
            "Saved 252 days for AAPL\n",
            "Fetching stock data for MSFT...\n",
            "Saved 252 days for MSFT\n",
            "Fetching stock data for GOOGL...\n",
            "Saved 252 days for GOOGL\n",
            "Fetching stock data for TSLA...\n",
            "Saved 252 days for TSLA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyflink.table import EnvironmentSettings, TableEnvironment\n",
        "from pyflink.table.udf import udf\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "\n",
        "def preprocess_reddit(sector):\n",
        "    \"\"\"Process Reddit posts with Flink to compute daily sentiment in batch mode.\"\"\"\n",
        "    logger.info(f\"Preprocessing Reddit data for {sector}\")\n",
        "    env_settings = EnvironmentSettings.in_batch_mode()\n",
        "    t_env = TableEnvironment.create(env_settings)\n",
        "\n",
        "    @udf(result_type=\"MAP<STRING, STRING>\")\n",
        "    def parse_json(line):\n",
        "        \"\"\"Parse JSON string into a map.\"\"\"\n",
        "        try:\n",
        "            data = json.loads(line)\n",
        "            return {k: str(v) for k, v in data.items()}\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    @udf(result_type=\"FLOAT\")\n",
        "    def get_vader_sentiment(text):\n",
        "        try:\n",
        "            return sid.polarity_scores(text or \"\")[\"compound\"]\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    @udf(result_type=\"STRING\")\n",
        "    def to_trading_date(timestamp):\n",
        "        try:\n",
        "            date = pd.to_datetime(float(timestamp), unit=\"s\")\n",
        "            return str(next_trading_day(date))\n",
        "        except:\n",
        "            return str(end_date)\n",
        "\n",
        "    t_env.create_temporary_function(\"parse_json\", parse_json)\n",
        "    t_env.create_temporary_function(\"get_vader_sentiment\", get_vader_sentiment)\n",
        "    t_env.create_temporary_function(\"to_trading_date\", to_trading_date)\n",
        "\n",
        "    reddit_file = f\"/content/reddit_{sector}_large.jsonl\"\n",
        "    if not os.path.exists(reddit_file):\n",
        "        logger.warning(f\"No Reddit data for {sector}\")\n",
        "        print(f\"No Reddit data for {sector}\")\n",
        "        return\n",
        "\n",
        "    t_env.execute_sql(\"\"\"\n",
        "        CREATE TABLE reddit_source (\n",
        "            line STRING\n",
        "        ) WITH (\n",
        "            'connector' = 'filesystem',\n",
        "            'path' = 'file://%s',\n",
        "            'format' = 'raw'\n",
        "        )\n",
        "    \"\"\" % reddit_file)\n",
        "\n",
        "    t_env.execute_sql(\"\"\"\n",
        "        CREATE TEMPORARY VIEW parsed_reddit AS\n",
        "        SELECT\n",
        "            parsed['company'] AS company,\n",
        "            to_trading_date(parsed['timestamp']) AS trading_date,\n",
        "            get_vader_sentiment(parsed['text']) AS sentiment_score\n",
        "        FROM (\n",
        "            SELECT parse_json(line) AS parsed\n",
        "            FROM reddit_source\n",
        "            WHERE line IS NOT NULL\n",
        "        ) t\n",
        "        WHERE parsed['company'] IS NOT NULL AND parsed['timestamp'] IS NOT NULL\n",
        "    \"\"\")\n",
        "\n",
        "    agg_table = t_env.sql_query(\"\"\"\n",
        "        SELECT\n",
        "            company,\n",
        "            trading_date,\n",
        "            AVG(sentiment_score) AS avg_sentiment,\n",
        "            COUNT(*) AS post_count\n",
        "        FROM parsed_reddit\n",
        "        GROUP BY company, trading_date\n",
        "    \"\"\")\n",
        "\n",
        "    sink_file = f\"/content/daily_sentiment_{sector}.csv\"\n",
        "    t_env.execute_sql(\"\"\"\n",
        "        CREATE TABLE sentiment_sink (\n",
        "            company STRING,\n",
        "            trading_date STRING,\n",
        "            avg_sentiment FLOAT,\n",
        "            post_count BIGINT\n",
        "        ) WITH (\n",
        "            'connector' = 'filesystem',\n",
        "            'path' = 'file://%s',\n",
        "            'format' = 'csv'\n",
        "        )\n",
        "    \"\"\" % sink_file)\n",
        "\n",
        "    agg_table.execute_insert(\"sentiment_sink\").wait()\n",
        "    logger.info(f\"Processed Reddit posts to {sink_file}\")\n",
        "    print(f\"Processed Reddit posts for {sector}\")\n",
        "\n",
        "def preprocess_stock(sector):\n",
        "    \"\"\"Process stock data with Flink for trends.\"\"\"\n",
        "    logger.info(f\"Preprocessing stock data for {sector}\")\n",
        "    env_settings = EnvironmentSettings.in_batch_mode()\n",
        "    t_env = TableEnvironment.create(env_settings)\n",
        "\n",
        "    companies = sectors[sector]\n",
        "    for company in companies:\n",
        "        cache_file = f\"/content/data/stock_{company}.csv\"\n",
        "        if not os.path.exists(cache_file):\n",
        "            logger.warning(f\"Stock data missing for {company}\")\n",
        "            print(f\"Stock data missing for {company}\")\n",
        "            continue\n",
        "\n",
        "        source_table = f\"stock_source_{company}\"\n",
        "        t_env.execute_sql(\"\"\"\n",
        "            CREATE TABLE %s (\n",
        "                `Date` STRING,\n",
        "                `Open` DOUBLE,\n",
        "                `Close` DOUBLE,\n",
        "                `High` DOUBLE,\n",
        "                `Low` DOUBLE,\n",
        "                `Volume` BIGINT\n",
        "            ) WITH (\n",
        "                'connector' = 'filesystem',\n",
        "                'path' = 'file://%s',\n",
        "                'format' = 'csv',\n",
        "                'csv.ignore-parse-errors' = 'true'\n",
        "            )\n",
        "        \"\"\" % (source_table, cache_file))\n",
        "\n",
        "        table = t_env.sql_query(\"\"\"\n",
        "            SELECT\n",
        "                `Date` AS `date`,\n",
        "                `Close` AS `close`,\n",
        "                CASE\n",
        "                    WHEN LEAD(`Close`) OVER (ORDER BY `Date`) > `Close` THEN 1\n",
        "                    ELSE 0\n",
        "                END AS `trend`\n",
        "            FROM %s\n",
        "            WHERE `Date` IS NOT NULL\n",
        "        \"\"\" % source_table)\n",
        "\n",
        "        sink_file = f\"/content/processed_stock_{company}.csv\"\n",
        "        sink_table = f\"stock_sink_{company}\"\n",
        "        t_env.execute_sql(\"\"\"\n",
        "            CREATE TABLE %s (\n",
        "                `date` STRING,\n",
        "                `close` DOUBLE,\n",
        "                `trend` INT\n",
        "            ) WITH (\n",
        "                'connector' = 'filesystem',\n",
        "                'path' = 'file://%s',\n",
        "                'format' = 'csv'\n",
        "            )\n",
        "        \"\"\" % (sink_table, sink_file))\n",
        "\n",
        "        table.execute_insert(sink_table).wait()\n",
        "        logger.info(f\"Processed stock data to {sink_file}\")\n",
        "        print(f\"Processed stock data for {company}\")\n",
        "\n",
        "preprocess_reddit(sector=\"Technology\")\n",
        "preprocess_stock(sector=\"Technology\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNhiJ50c596v",
        "outputId": "3e85985a-82a7-4540-ab91-ce95f0cb8368"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Reddit posts for Technology\n",
            "Processed stock data for AAPL\n",
            "Processed stock data for MSFT\n",
            "Processed stock data for GOOGL\n",
            "Processed stock data for TSLA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import logging\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Assuming logger, sectors from Segment 2\n",
        "logger.info(\"Starting LSTM data preparation\")\n",
        "\n",
        "def prepare_lstm_data(sector, sequence_length=60):\n",
        "    \"\"\"Prepare sequences for LSTM from sentiment and stock data.\"\"\"\n",
        "    logger.info(f\"Preparing LSTM data for {sector}\")\n",
        "\n",
        "    # Find sentiment part file\n",
        "    sentiment_dir = f\"/content/daily_sentiment_{sector}.csv\"\n",
        "    sentiment_files = glob.glob(f\"{sentiment_dir}/part-*\")\n",
        "    if not sentiment_files:\n",
        "        logger.error(f\"No part files found in {sentiment_dir}\")\n",
        "        print(f\"No part files found in {sentiment_dir}\")\n",
        "        return []\n",
        "\n",
        "    sentiment_part_file = sentiment_files[0]  # Take first part file\n",
        "    if not os.path.isfile(sentiment_part_file):\n",
        "        logger.error(f\"Sentiment part file is not a file: {sentiment_part_file}\")\n",
        "        print(f\"Sentiment part file is not a file: {sentiment_part_file}\")\n",
        "        return []\n",
        "\n",
        "    # Read sentiment data with explicit column names\n",
        "    try:\n",
        "        sentiment_df = pd.read_csv(\n",
        "            sentiment_part_file,\n",
        "            names=['company', 'trading_date', 'avg_sentiment', 'post_count'],\n",
        "            header=None\n",
        "        )\n",
        "        logger.info(f\"Loaded sentiment data: {len(sentiment_df)} rows\")\n",
        "        print(f\"Loaded sentiment data: {len(sentiment_df)} rows\")\n",
        "        print(f\"Sentiment columns: {list(sentiment_df.columns)}\")\n",
        "        print(\"Sentiment sample:\")\n",
        "        print(sentiment_df.head().to_string())\n",
        "        unique_companies = sentiment_df['company'].unique()\n",
        "        print(f\"Unique companies: {unique_companies}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to read sentiment part file {sentiment_part_file}: {e}\")\n",
        "        print(f\"Failed to read sentiment part file: {e}\")\n",
        "        return []\n",
        "\n",
        "    # Validate sentiment columns\n",
        "    required_cols = ['company', 'trading_date', 'avg_sentiment', 'post_count']\n",
        "    missing_cols = [col for col in required_cols if col not in sentiment_df.columns]\n",
        "    if missing_cols:\n",
        "        logger.error(f\"Missing required columns in sentiment data: {missing_cols}\")\n",
        "        print(f\"Missing required columns in sentiment data: {missing_cols}\")\n",
        "        return []\n",
        "\n",
        "    companies = sectors[sector]\n",
        "    sequences = []\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    for company in companies:\n",
        "        # Find stock part file\n",
        "        stock_dir = f\"/content/processed_stock_{company}.csv\"\n",
        "        stock_files = glob.glob(f\"{stock_dir}/part-*\")\n",
        "        if not stock_files:\n",
        "            logger.warning(f\"No part files found in {stock_dir}\")\n",
        "            print(f\"No part files found for {company}\")\n",
        "            continue\n",
        "\n",
        "        stock_part_file = stock_files[0]  # Take first part file\n",
        "        if not os.path.isfile(stock_part_file):\n",
        "            logger.warning(f\"Stock part file is not a file: {stock_part_file}\")\n",
        "            print(f\"Stock part file is not a file for {company}\")\n",
        "            continue\n",
        "\n",
        "        # Read stock data with explicit column names\n",
        "        try:\n",
        "            stock_df = pd.read_csv(\n",
        "                stock_part_file,\n",
        "                names=['date', 'close', 'trend'],\n",
        "                header=None\n",
        "            )\n",
        "            logger.info(f\"Loaded stock data for {company}: {len(stock_df)} rows\")\n",
        "            print(f\"Loaded stock data for {company}: {len(stock_df)} rows\")\n",
        "            print(f\"Stock columns for {company}: {list(stock_df.columns)}\")\n",
        "            print(f\"Stock sample for {company}:\")\n",
        "            print(stock_df.head().to_string())\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to read stock part file {stock_part_file}: {e}\")\n",
        "            print(f\"Failed to read stock part file for {company}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Validate stock columns\n",
        "        required_stock_cols = ['date', 'close', 'trend']\n",
        "        missing_stock_cols = [col for col in required_stock_cols if col not in stock_df.columns]\n",
        "        if missing_stock_cols:\n",
        "            logger.error(f\"Missing required columns in stock data for {company}: {missing_stock_cols}\")\n",
        "            print(f\"Missing required columns in stock data for {company}: {missing_stock_cols}\")\n",
        "            continue\n",
        "\n",
        "        # Merge data\n",
        "        try:\n",
        "            company_sentiment = sentiment_df[sentiment_df['company'] == company][\n",
        "                ['trading_date', 'avg_sentiment', 'post_count']\n",
        "            ]\n",
        "            merged_df = stock_df.merge(\n",
        "                company_sentiment,\n",
        "                left_on='date',\n",
        "                right_on='trading_date',\n",
        "                how='left'\n",
        "            )\n",
        "            merged_df['avg_sentiment'] = merged_df['avg_sentiment'].fillna(0.0)\n",
        "            merged_df['post_count'] = merged_df['post_count'].fillna(0)\n",
        "            merged_df = merged_df.drop(columns=['trading_date'], errors='ignore')\n",
        "            logger.info(f\"Merged data for {company}: {len(merged_df)} rows\")\n",
        "            print(f\"Merged data for {company}: {len(merged_df)} rows\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to merge data for {company}: {e}\")\n",
        "            print(f\"Failed to merge data for {company}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Prepare features and target\n",
        "        try:\n",
        "            features = merged_df[['close', 'avg_sentiment', 'post_count']].values\n",
        "            target = merged_df['trend'].values\n",
        "\n",
        "            if len(features) > 0:\n",
        "                scaled_features = scaler.fit_transform(features)\n",
        "            else:\n",
        "                logger.warning(f\"No features for {company}\")\n",
        "                print(f\"No features for {company}\")\n",
        "                continue\n",
        "\n",
        "            X, y = [], []\n",
        "            for i in range(len(scaled_features) - sequence_length):\n",
        "                X.append(scaled_features[i:i + sequence_length])\n",
        "                y.append(target[i + sequence_length])\n",
        "\n",
        "            if X:\n",
        "                sequences.append((company, np.array(X), np.array(y)))\n",
        "                logger.info(f\"Created {len(X)} sequences for {company}\")\n",
        "                print(f\"Created {len(X)} sequences for {company}\")\n",
        "            else:\n",
        "                logger.warning(f\"No sequences for {company}\")\n",
        "                print(f\"No sequences for {company}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to create sequences for {company}: {e}\")\n",
        "            print(f\"Failed to create sequences for {company}: {e}\")\n",
        "\n",
        "    total_sequences = sum(len(X) for _, X, _ in sequences)\n",
        "    logger.info(f\"Total sequences prepared: {total_sequences}\")\n",
        "    print(f\"Total sequences prepared: {total_sequences}\")\n",
        "    return sequences\n",
        "\n",
        "# Run for Technology sector\n",
        "lstm_data = prepare_lstm_data(\"Technology\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Pme1KP8_ifN",
        "outputId": "64faacc2-4425-4169-8b30-06b03a28d787"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded sentiment data: 713 rows\n",
            "Sentiment columns: ['company', 'trading_date', 'avg_sentiment', 'post_count']\n",
            "Sentiment sample:\n",
            "  company trading_date  avg_sentiment  post_count\n",
            "0    AAPL   2025-04-14       0.264200           3\n",
            "1    AAPL   2025-04-07       0.453400           6\n",
            "2    AAPL   2025-04-04       0.936633           3\n",
            "3    AAPL   2025-03-03       0.299175           4\n",
            "4    AAPL   2025-02-27       0.458400           2\n",
            "Unique companies: ['AAPL' 'MSFT' 'GOOGL' 'TSLA']\n",
            "Loaded stock data for AAPL: 253 rows\n",
            "Stock columns for AAPL: ['date', 'close', 'trend']\n",
            "Stock sample for AAPL:\n",
            "         date       close  trend\n",
            "0  2024-04-16  168.583984      0\n",
            "1  2024-04-17  167.210464      0\n",
            "2  2024-04-18  166.254959      0\n",
            "3  2024-04-19  164.224564      1\n",
            "4  2024-04-22  165.060593      1\n",
            "Merged data for AAPL: 253 rows\n",
            "Created 193 sequences for AAPL\n",
            "Loaded stock data for MSFT: 253 rows\n",
            "Stock columns for MSFT: ['date', 'close', 'trend']\n",
            "Stock sample for MSFT:\n",
            "         date       close  trend\n",
            "0  2024-04-16  411.438812      0\n",
            "1  2024-04-17  408.719543      0\n",
            "2  2024-04-18  401.206940      0\n",
            "3  2024-04-19  396.095947      1\n",
            "4  2024-04-22  397.921997      1\n",
            "Merged data for MSFT: 253 rows\n",
            "Created 193 sequences for MSFT\n",
            "Loaded stock data for GOOGL: 253 rows\n",
            "Stock columns for GOOGL: ['date', 'close', 'trend']\n",
            "Stock sample for GOOGL:\n",
            "         date       close  trend\n",
            "0  2024-04-16  153.665329      1\n",
            "1  2024-04-17  154.730255      1\n",
            "2  2024-04-18  155.267685      0\n",
            "3  2024-04-19  153.356796      1\n",
            "4  2024-04-22  155.536377      1\n",
            "Merged data for GOOGL: 253 rows\n",
            "Created 193 sequences for GOOGL\n",
            "Loaded stock data for TSLA: 253 rows\n",
            "Stock columns for TSLA: ['date', 'close', 'trend']\n",
            "Stock sample for TSLA:\n",
            "         date       close  trend\n",
            "0  2024-04-16  157.110001      0\n",
            "1  2024-04-17  155.449997      0\n",
            "2  2024-04-18  149.929993      0\n",
            "3  2024-04-19  147.050003      0\n",
            "4  2024-04-22  142.050003      1\n",
            "Merged data for TSLA: 253 rows\n",
            "Created 193 sequences for TSLA\n",
            "Total sequences prepared: 772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Assuming logger from Segment 2\n",
        "logger.info(\"Starting LSTM training\")\n",
        "\n",
        "def train_lstm_models(lstm_data, epochs=10, batch_size=32):\n",
        "    \"\"\"Train LSTM models for each company.\"\"\"\n",
        "    models = {}\n",
        "\n",
        "    for company, X, y in lstm_data:\n",
        "        logger.info(f\"Training LSTM for {company}\")\n",
        "        print(f\"Training LSTM for {company}\")\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "        logger.info(f\"{company} - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "        print(f\"{company} - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "        # Build LSTM model\n",
        "        model = Sequential([\n",
        "            LSTM(50, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),\n",
        "            Dropout(0.2),\n",
        "            LSTM(50),\n",
        "            Dropout(0.2),\n",
        "            Dense(25, activation='relu'),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(X_test, y_test),\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Evaluate model\n",
        "        loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "        logger.info(f\"{company} - Test accuracy: {accuracy:.4f}\")\n",
        "        print(f\"{company} - Test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Save model\n",
        "        model_path = f\"/content/model_{company}.h5\"\n",
        "        model.save(model_path)\n",
        "        logger.info(f\"Saved model for {company} to {model_path}\")\n",
        "        print(f\"Saved model for {company} to {model_path}\")\n",
        "\n",
        "        models[company] = model\n",
        "\n",
        "    return models\n",
        "\n",
        "# Train models using Segment 6 output\n",
        "models = train_lstm_models(lstm_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkQM4Et_DrQK",
        "outputId": "85f8e9fb-17fd-4527-e9a5-eb559c02b66f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM for AAPL\n",
            "AAPL - Train: (154, 60, 3), Test: (39, 60, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 442ms/step - accuracy: 0.4033 - loss: 0.7013 - val_accuracy: 0.5897 - val_loss: 0.6875\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.5052 - loss: 0.6935 - val_accuracy: 0.5897 - val_loss: 0.6876\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.5234 - loss: 0.6942 - val_accuracy: 0.5897 - val_loss: 0.6864\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.5521 - loss: 0.6901 - val_accuracy: 0.5897 - val_loss: 0.6843\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.5429 - loss: 0.6888 - val_accuracy: 0.5897 - val_loss: 0.6842\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5464 - loss: 0.6890 - val_accuracy: 0.5897 - val_loss: 0.6850\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5495 - loss: 0.6911 - val_accuracy: 0.5897 - val_loss: 0.6863\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.4753 - loss: 0.6967 - val_accuracy: 0.5897 - val_loss: 0.6910\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.4527 - loss: 0.6942 - val_accuracy: 0.5385 - val_loss: 0.6928\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.4965 - loss: 0.6944 - val_accuracy: 0.5897 - val_loss: 0.6901\n",
            "AAPL - Test accuracy: 0.5897\n",
            "Saved model for AAPL to /content/model_AAPL.h5\n",
            "Training LSTM for MSFT\n",
            "MSFT - Train: (154, 60, 3), Test: (39, 60, 3)\n",
            "Epoch 1/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 190ms/step - accuracy: 0.5347 - loss: 0.6933 - val_accuracy: 0.5385 - val_loss: 0.6916\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.5208 - loss: 0.6908 - val_accuracy: 0.5385 - val_loss: 0.6923\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.4844 - loss: 0.6927 - val_accuracy: 0.4615 - val_loss: 0.6951\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.4579 - loss: 0.6964 - val_accuracy: 0.4615 - val_loss: 0.6954\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.5260 - loss: 0.6944 - val_accuracy: 0.4615 - val_loss: 0.6948\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.5148 - loss: 0.6923 - val_accuracy: 0.4615 - val_loss: 0.6948\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.4878 - loss: 0.6927 - val_accuracy: 0.4615 - val_loss: 0.6940\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.5772 - loss: 0.6911 - val_accuracy: 0.4615 - val_loss: 0.6949\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.5043 - loss: 0.6923 - val_accuracy: 0.4615 - val_loss: 0.6941\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.5490 - loss: 0.6921 - val_accuracy: 0.4103 - val_loss: 0.6935\n",
            "MSFT - Test accuracy: 0.4103\n",
            "Saved model for MSFT to /content/model_MSFT.h5\n",
            "Training LSTM for GOOGL\n",
            "GOOGL - Train: (154, 60, 3), Test: (39, 60, 3)\n",
            "Epoch 1/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 298ms/step - accuracy: 0.4518 - loss: 0.6985 - val_accuracy: 0.5641 - val_loss: 0.6916\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - accuracy: 0.4761 - loss: 0.6929 - val_accuracy: 0.5128 - val_loss: 0.6927\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.4757 - loss: 0.6934 - val_accuracy: 0.5385 - val_loss: 0.6931\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5438 - loss: 0.6903 - val_accuracy: 0.3846 - val_loss: 0.7030\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5039 - loss: 0.6951 - val_accuracy: 0.3846 - val_loss: 0.7043\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4978 - loss: 0.6954 - val_accuracy: 0.3846 - val_loss: 0.6998\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5148 - loss: 0.6895 - val_accuracy: 0.3846 - val_loss: 0.7017\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4921 - loss: 0.6944 - val_accuracy: 0.4103 - val_loss: 0.6973\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.4930 - loss: 0.6918 - val_accuracy: 0.4103 - val_loss: 0.6954\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.5139 - loss: 0.6882 - val_accuracy: 0.4103 - val_loss: 0.6962\n",
            "GOOGL - Test accuracy: 0.4103\n",
            "Saved model for GOOGL to /content/model_GOOGL.h5\n",
            "Training LSTM for TSLA\n",
            "TSLA - Train: (154, 60, 3), Test: (39, 60, 3)\n",
            "Epoch 1/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 185ms/step - accuracy: 0.4701 - loss: 0.6966 - val_accuracy: 0.5385 - val_loss: 0.6901\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.4622 - loss: 0.6998 - val_accuracy: 0.5385 - val_loss: 0.6899\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5351 - loss: 0.6911 - val_accuracy: 0.5385 - val_loss: 0.6876\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4865 - loss: 0.6944 - val_accuracy: 0.5385 - val_loss: 0.6863\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - accuracy: 0.5152 - loss: 0.6891 - val_accuracy: 0.5385 - val_loss: 0.6844\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 0.5364 - loss: 0.6839 - val_accuracy: 0.5385 - val_loss: 0.6819\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.5169 - loss: 0.6872 - val_accuracy: 0.5385 - val_loss: 0.6801\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.5299 - loss: 0.6881 - val_accuracy: 0.5385 - val_loss: 0.6791\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step - accuracy: 0.5681 - loss: 0.6904 - val_accuracy: 0.5641 - val_loss: 0.6800\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - accuracy: 0.5494 - loss: 0.6875 - val_accuracy: 0.6154 - val_loss: 0.6835\n",
            "TSLA - Test accuracy: 0.6154\n",
            "Saved model for TSLA to /content/model_TSLA.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Assuming logger, sectors from Segment 2\n",
        "logger.info(\"Starting trend predictions\")\n",
        "\n",
        "def predict_trends(lstm_data, models, forecast_days=10):\n",
        "    \"\"\"Predict stock trends using trained LSTM models.\"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for company, X, _ in lstm_data:\n",
        "        logger.info(f\"Predicting trends for {company}\")\n",
        "        print(f\"Predicting trends for {company}\")\n",
        "\n",
        "        if company not in models:\n",
        "            logger.warning(f\"No model found for {company}\")\n",
        "            print(f\"No model found for {company}\")\n",
        "            continue\n",
        "\n",
        "        model = models[company]\n",
        "        # Use latest 60-day sequence for prediction\n",
        "        latest_sequence = X[-1:]  # Shape: [1, 60, 3]\n",
        "\n",
        "        # Predict for next forecast_days\n",
        "        company_predictions = []\n",
        "        current_sequence = latest_sequence.copy()\n",
        "        last_date = datetime.strptime(\"2025-04-16\", \"%Y-%m-%d\")  # Latest stock date\n",
        "\n",
        "        for day in range(forecast_days):\n",
        "            # Predict probability\n",
        "            pred_prob = model.predict(current_sequence, verbose=0)[0][0]\n",
        "            trend = 1 if pred_prob >= 0.5 else 0  # Threshold at 0.5\n",
        "            pred_date = last_date + timedelta(days=day + 1)\n",
        "\n",
        "            company_predictions.append({\n",
        "                'company': company,\n",
        "                'date': pred_date.strftime(\"%Y-%m-%d\"),\n",
        "                'trend': trend,\n",
        "                'probability': float(pred_prob)\n",
        "            })\n",
        "\n",
        "            # Update sequence (shift and append dummy data for demo)\n",
        "            # In practice, use actual new data if available\n",
        "            new_data = current_sequence[:, -1, :].copy()\n",
        "            new_data[:, 0] = new_data[:, 0]  # Keep last close (dummy)\n",
        "            new_data[:, 1] = 0.0  # Neutral sentiment\n",
        "            new_data[:, 2] = 0  # No posts\n",
        "            current_sequence = np.append(current_sequence[:, 1:, :], [new_data], axis=1)\n",
        "\n",
        "        predictions.extend(company_predictions)\n",
        "        logger.info(f\"Predicted {len(company_predictions)} trends for {company}\")\n",
        "        print(f\"Predicted {len(company_predictions)} trends for {company}\")\n",
        "\n",
        "    # Save predictions\n",
        "    predictions_df = pd.DataFrame(predictions)\n",
        "    output_path = \"/content/predictions.csv\"\n",
        "    predictions_df.to_csv(output_path, index=False)\n",
        "    logger.info(f\"Saved predictions to {output_path}\")\n",
        "    print(f\"Saved predictions to {output_path}\")\n",
        "\n",
        "    return predictions_df\n",
        "\n",
        "# Load models\n",
        "models = {}\n",
        "for company in ['AAPL', 'MSFT', 'GOOGL', 'TSLA']:\n",
        "    model_path = f\"/content/model_{company}.h5\"\n",
        "    if os.path.exists(model_path):\n",
        "        models[company] = tf.keras.models.load_model(model_path)\n",
        "        print(f\"Loaded model for {company}\")\n",
        "    else:\n",
        "        print(f\"Model not found for {company}\")\n",
        "\n",
        "# Predict trends using Segment 6's lstm_data\n",
        "predictions_df = predict_trends(lstm_data, models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzqzM7PUEy8f",
        "outputId": "a1cfbe60-bd39-49ef-a811-d66493f36132"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model for AAPL\n",
            "Loaded model for MSFT\n",
            "Loaded model for GOOGL\n",
            "Loaded model for TSLA\n",
            "Predicting trends for AAPL\n",
            "Predicted 10 trends for AAPL\n",
            "Predicting trends for MSFT\n",
            "Predicted 10 trends for MSFT\n",
            "Predicting trends for GOOGL\n",
            "Predicted 10 trends for GOOGL\n",
            "Predicting trends for TSLA\n",
            "Predicted 10 trends for TSLA\n",
            "Saved predictions to /content/predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "# Assuming logger from Segment 2\n",
        "logger.info(\"Starting alert generation\")\n",
        "\n",
        "def generate_alerts(predictions_file=\"/content/predictions.csv\"):\n",
        "    \"\"\"Generate alerts based on prediction probabilities.\"\"\"\n",
        "    try:\n",
        "        predictions_df = pd.read_csv(predictions_file)\n",
        "        logger.info(f\"Loaded predictions: {len(predictions_df)} rows\")\n",
        "        print(f\"Loaded predictions: {len(predictions_df)} rows\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to read predictions: {e}\")\n",
        "        print(f\"Failed to read predictions: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    alerts = []\n",
        "    for _, row in predictions_df.iterrows():\n",
        "        company = row['company']\n",
        "        date = row['date']\n",
        "        probability = row['probability']\n",
        "        trend = row['trend']\n",
        "\n",
        "        # Alert thresholds\n",
        "        if probability >= 0.7 and trend == 1:\n",
        "            alert_type = \"Strong Buy\"\n",
        "        elif probability <= 0.3 and trend == 0:\n",
        "            alert_type = \"Strong Sell\"\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        alerts.append({\n",
        "            'company': company,\n",
        "            'date': date,\n",
        "            'alert_type': alert_type,\n",
        "            'probability': probability\n",
        "        })\n",
        "\n",
        "    alerts_df = pd.DataFrame(alerts)\n",
        "    output_path = \"/content/alerts.csv\"\n",
        "    alerts_df.to_csv(output_path, index=False)\n",
        "    logger.info(f\"Generated {len(alerts_df)} alerts, saved to {output_path}\")\n",
        "    print(f\"Generated {len(alerts_df)} alerts, saved to {output_path}\")\n",
        "\n",
        "    return alerts_df\n",
        "\n",
        "# Generate alerts\n",
        "alerts_df = generate_alerts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX3Q6vPeE3J7",
        "outputId": "525159c2-f456-45d7-c46a-63f9769294c2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded predictions: 40 rows\n",
            "Generated 0 alerts, saved to /content/alerts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit==1.43.2 pyngrok plotly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaOzqWi2GvF2",
        "outputId": "51259dec-8bd2-40d2-bb55-7b34f6b9c10d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit==1.43.2 in /usr/local/lib/python3.11/dist-packages (1.43.2)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (16.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (4.13.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.43.2) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit==1.43.2) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit==1.43.2) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit==1.43.2) (1.34.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.43.2) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit==1.43.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit==1.43.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit==1.43.2) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit==1.43.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit==1.43.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit==1.43.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit==1.43.2) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.43.2) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit==1.43.2) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.43.2) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.43.2) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.43.2) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.43.2) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit==1.43.2) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2tXLBQHCEqsQS0TYDRNHF5w8fHk_2rbtDMMHkdDsLPdss1hUP\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af9BhxtEG4Y1",
        "outputId": "71d13c4e-7523-4681-f62d-bf7445caae64"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/dashboard.py\", \"w\") as f:\n",
        "    f.write('''<import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import logging\n",
        "\n",
        "# Assuming logger from Segment 2\n",
        "logger.info(\"Starting Streamlit dashboard\")\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Stock Sentiment Dashboard\")\n",
        "\n",
        "# Load data\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    try:\n",
        "        predictions = pd.read_csv(\"/content/predictions.csv\")\n",
        "        alerts = pd.read_csv(\"/content/alerts.csv\")\n",
        "        return predictions, alerts\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load data: {e}\")\n",
        "        st.error(f\"Failed to load data: {e}\")\n",
        "        return None, None\n",
        "\n",
        "predictions, alerts = load_data()\n",
        "\n",
        "if predictions is not None and alerts is not None:\n",
        "    # Sidebar for company selection\n",
        "    companies = predictions['company'].unique()\n",
        "    selected_company = st.sidebar.selectbox(\"Select Company\", companies)\n",
        "\n",
        "    # Predictions table\n",
        "    st.subheader(f\"Predictions for {selected_company}\")\n",
        "    company_predictions = predictions[predictions['company'] == selected_company]\n",
        "    st.dataframe(company_predictions[['date', 'trend', 'probability']])\n",
        "\n",
        "    # Predictions plot\n",
        "    fig = px.line(\n",
        "        company_predictions,\n",
        "        x='date',\n",
        "        y='probability',\n",
        "        title=f\"Trend Probability for {selected_company}\",\n",
        "        labels={'probability': 'Trend Probability', 'date': 'Date'}\n",
        "    )\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # Alerts table\n",
        "    st.subheader(f\"Alerts for {selected_company}\")\n",
        "    company_alerts = alerts[alerts['company'] == selected_company]\n",
        "    if not company_alerts.empty:\n",
        "        st.dataframe(company_alerts[['date', 'alert_type', 'probability']])\n",
        "    else:\n",
        "        st.write(\"No alerts for this company.\")\n",
        "else:\n",
        "    st.write(\"Data loading failed. Check logs.\")>''')"
      ],
      "metadata": {
        "id": "AieYxh3mHHIS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start Streamlit server\n",
        "streamlit_proc = subprocess.Popen([\"streamlit\", \"run\", \"/content/dashboard.py\", \"--server.port\", \"8501\"])\n",
        "time.sleep(5)  # Wait for Streamlit to start\n",
        "\n",
        "# Create ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit dashboard running at: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpvRvJgKFOzJ",
        "outputId": "8984f3df-ccf5-4546-aeba-aa5c8299f5f9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit dashboard running at: NgrokTunnel: \"https://4f32-34-55-42-235.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}